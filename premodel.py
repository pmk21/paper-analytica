# -*- coding: utf-8 -*-
"""premodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_MFcntj43CAWeX0DhDi3ptkKziH3HY_H

# Performing EDA and Pre-processing on the Dataset

## Loading required packages
"""

import pandas as pd
import json
import operator
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel



"""## Preprocessing the data"""

DATA_DIR = "./"

with open(DATA_DIR + "arxivData.json", "r") as fp:
    data = json.load(fp)
data[0]

def str_to_list(x):
    """
        Converting string enclosed lists to a python list
    """
    for i in range(len(x)):
        x[i]["author"] = eval(x[i]["author"])
        # x[i]["link"] = eval(x[i]["link"])
        x[i]["tag"] = eval(x[i]["tag"])

data[0]
str_to_list(data)

"""## Data Visualization

* Papers published per year
"""

summaries = list(map(lambda x:x["summary"], data))
idSummaryMap = list(map(lambda x:(x["id"], x["summary"]), data))
text_summ = ' '.join(summaries)
len(summaries)
idSummaryMap[0][0]

def preprocess_text(text):
    """
        Preprocess text data for performing analysis
    """
    # Converting all the letters to lowercase
    text = text.lower()
    # Removing numbers from the text
    text = re.sub(r'\d+', '', text)
    # Removing punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Removing trailing and leading whitespaces
    text = text.strip(' ')
    # Removing stop words
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filt_sum = [w for w in word_tokens if w not in stop_words]
    
    return filt_sum

processedSummaries = preprocess_text(text_summ)

#summaries to a tfidf vector 
tf = TfidfVectorizer(tokenizer = word_tokenize, stop_words='english')
tfidf_matrix = tf.fit_transform(processedSummaries)

#the kernel is applied on the processed summary to make the computation feasible by reducing the size of the sparse matrix, but the result will have the complete summary
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)
result = {}
for idx, row in enumerate(summaries):
#       print(cosine_similarities[idx])

      related_indices = cosine_similarities[idx].argsort()[:-10:-1]#10 most related docs
      similar_items = [(cosine_similarities[idx][i], idSummaryMap[i][1]) for i in related_indices] 
      result[idSummaryMap[idx][0]] = similar_items[1:]

print(result)